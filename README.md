# 🔍 Explainable BERT (Under Development)

⚠️ **NOTE:** This is a custom research-driven project aiming to enhance explainability in Transformer-based architectures. A minor but **novel architectural change** has been made to standard BERT in order to **highlight the tokens most responsible for a given prediction** (e.g., in sentiment classification).

💡 The **core modification** will be detailed after full development is complete and the model is evaluated extensively. A **prototype implementation** has already shown **promising results** — improving interpretability while maintaining or even increasing accuracy over the baseline.

🛠️ This repo is currently in active development. Stay tuned!

---

## 🚀 Goals

- Build a lightweight BERT model with built-in interpretability
- Highlight token-level contributions during inference
- Compare against standard multi-head attention strategies
- Open source the implementation after stability & documentation


